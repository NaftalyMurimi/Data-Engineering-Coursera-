{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97e97928",
   "metadata": {},
   "source": [
    "# Web Scraping with python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2dd1980",
   "metadata": {},
   "source": [
    "Web scraping is used for extraction of relevant data from web pages. \n",
    "If you require some data from a \n",
    "web page in a public domain, web scraping makes the process of data extraction quite convenient.\n",
    "The use of web scraping, however, requires some basic knowledge of the structure of HTML pages. \n",
    "In this lab, you will learn the process of analyzing the HTML code of a web page and how to extract the required information from it using web scraping in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256dd104",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "By the end of this lab, you will be able to:\n",
    "\n",
    "Use the requests and BeautifulSoup libraries to extract the contents of a web page\n",
    "\n",
    "Analyze the HTML code of a webpage to find the relevant information\n",
    "\n",
    "Extract the relevant information and save it in the required form"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1309eb",
   "metadata": {},
   "source": [
    "### libraries required\n",
    "\n",
    "pandas library for data storage and manipulation.\n",
    "\n",
    "BeautifulSoup library for interpreting the HTML document.\n",
    "\n",
    "requests library to communicate with the web page.\n",
    "\n",
    "sqlite3 for creating the database instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ce0e9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d8a7fb1",
   "metadata": {},
   "source": [
    "### Initialization of known entities\n",
    "You must declare a few entities at the beginning. For example, you know the required URL, the CSV name for saving the record, the database name, and the table name for storing the record. You also know the entities to be saved. Additionally, since you require only the top 50 results, you will require a loop counter initialized to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71763f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://web.archive.org/web/20230902185655/https://en.everybodywiki.com/100_Most_Highly-Ranked_Films'\n",
    "db_name = 'Movies.db'\n",
    "table_name = 'Top_50'\n",
    "csv_path = 'top_50_films.csv'\n",
    "df = pd.DataFrame(columns=[\"Average Rank\",\"Film\",\"Year\"])\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37d8b91",
   "metadata": {},
   "source": [
    "### Loading the webpage for Webscraping\n",
    "To access the required information from the web page, you first need to load the entire web page as an \n",
    "HTML document in python using the requests.get().text function and then parse the text in the HTML format using \n",
    "BeautifulSoup to enable extraction of relevant information\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e83401d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_page = requests.get(url).text\n",
    "data = BeautifulSoup(html_page, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f045028",
   "metadata": {},
   "source": [
    "### Scraping of required information\n",
    "You now need to write the loop to extract the appropriate information from the web page. \n",
    "The rows of the table needed can be accessed using the find_all() function with the BeautifulSoup object using the statements below.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68b4a7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = data.find_all('tbody')\n",
    "rows = tables[0].find_all('tr')\n",
    "#Here, the variable tables gets the body of all the tables in the web page and the variable\n",
    "# rows gets all the rows of the first table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e5a6a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in rows:\n",
    "    if count<50:\n",
    "        col = row.find_all('td')\n",
    "        if len(col)!=0:\n",
    "            data_dict = {\"Average Rank\": col[0].contents[0],\n",
    "                         \"Film\": col[1].contents[0],\n",
    "                         \"Year\": col[2].contents[0]}\n",
    "            df1 = pd.DataFrame(data_dict, index=[0])\n",
    "            df = pd.concat([df,df1], ignore_index=True)\n",
    "            count+=1\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d51a7d4",
   "metadata": {},
   "source": [
    "### Iterate over the contents of the variable rows.\n",
    "### Check for the loop counter to restrict to 50 entries.\n",
    "### Extract all the td data objects in the row and save them to col.\n",
    "### Check if the length of col is 0, that is, if there is no data in a current row. This is important since, many timesm there are merged rows that are not apparent in the web page appearance.\n",
    "### Create a dictionary data_dict with the keys same as the columns of the dataframe created for recording the output earlier and corresponding values from the first three headers of data.\n",
    "### Convert the dictionary to a dataframe and concatenate it with the existing one. This way, the data keeps getting appended to the dataframe with every iteration of the loop.\n",
    "### Increment the loop counter.\n",
    "### Once the counter hits 50, stop iterating over rows and break the loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb027a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Average Rank                                           Film  Year\n",
      "0             1                                  The Godfather  1972\n",
      "1             2                                   Citizen Kane  1941\n",
      "2             3                                     Casablanca  1942\n",
      "3             4                         The Godfather, Part II  1974\n",
      "4             5                            Singin' in the Rain  1952\n",
      "5             6                                         Psycho  1960\n",
      "6             7                                    Rear Window  1954\n",
      "7             8                                 Apocalypse Now  1979\n",
      "8             9                          2001: A Space Odyssey  1968\n",
      "9            10                                  Seven Samurai  1954\n",
      "10           11                                        Vertigo  1958\n",
      "11           12                                    Sunset Blvd  1950\n",
      "12           13                                   Modern Times  1936\n",
      "13           14                             Lawrence of Arabia  1962\n",
      "14           15                             North by Northwest  1959\n",
      "15           16                                      Star Wars  1977\n",
      "16           17                                       Parasite  2019\n",
      "17           18                               Schindler's List  1993\n",
      "18           19  Lord of the Rings: The Fellowship of the Ring  2001\n",
      "19           20                           Shawshank Redemption  1994\n",
      "20           21                          It's a Wonderful Life  1946\n",
      "21           22                                   Pulp Fiction  1994\n",
      "22           23                              Avengers: Endgame  2019\n",
      "23           24                                    City Lights  1931\n",
      "24           25                One Flew Over the Cuckoo's Nest  1975\n",
      "25           26                                     Goodfellas  1990\n",
      "26           27                        Raiders of the Lost Ark  1981\n",
      "27           28                                   12 Angry Men  1957\n",
      "28           29                       The Silence of the Lambs  1991\n",
      "29           30                                    Taxi Driver  1976\n",
      "30           31                            Saving Private Ryan  1998\n",
      "31           32                     E.T. the Extra Terrestrial  1982\n",
      "32           33                                          Alien  1979\n",
      "33           34              Spider-Man: Into the Spider-verse  2018\n",
      "34           35                                   Blade Runner  1982\n",
      "35           36                               Double Indemnity  1944\n",
      "36           37                                The Dark Knight  2008\n",
      "37           38                               The Wizard of Oz  1939\n",
      "38           39  Star Wars: Episode V- The Empire Strikes Back  1980\n",
      "39           40                                  The Searchers  1956\n",
      "40           41                             Mad Max: Fury Road  2015\n",
      "41           42                                      Inception  2010\n",
      "42           43          Lord of the Rings: Return of the King  2003\n",
      "43           44                                     The Matrix  1999\n",
      "44           45                                     Fight Club  1999\n",
      "45           46                             Back to the Future  1985\n",
      "46           47                          It Happened One Night  1934\n",
      "47           48                The Good, the Bad, and the Ugly  1966\n",
      "48           49              Lord of the Rings: The Two Towers  2002\n",
      "49           50                                  All About Eve  1950\n"
     ]
    }
   ],
   "source": [
    "#View the scrapped data\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b7a16a94",
   "metadata": {},
   "source": [
    "### Storing the data\n",
    "After the dataframe has been created, you can save it to a CSV file using the following command:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d6280f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05790cbc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
