{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5072c65",
   "metadata": {},
   "source": [
    "# Web-scrapping jobs from karatina university website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c18481f",
   "metadata": {},
   "source": [
    "### NOTE: This project is for learing purposes only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ad80d4",
   "metadata": {},
   "source": [
    "This Prpject aims at scrapping jobs listed at karatina university website. \n",
    "The details to be extracted are job Title, job Grade, No of positions to be filled Positions and Reference Number\n",
    "The data will then be stored in Karu_jobs.csv and karu_jobs.db files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0094d286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all neccessary modules\n",
    "import requests\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d03dd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the url to fetch data from\n",
    "url = \"https://karu.ac.ke/past-jobs-karatina-university-page/\"\n",
    "#Declare files to store extracted data\n",
    "karu_jobs = \"karu_jobs.csv\"\n",
    "karu_careers = \"karu_jobs.db\"\n",
    "# Define a dataframe variable to store job Title, job Grade, No of positions to be filled Positions and Reference Number\n",
    "df = pd.DataFrame(columns= [\"Job_Title\", \"Job_Grade\", \"No_Of_Positions\", \"Reference_No\"])\n",
    "count = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06f5d731",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Access the contents of the webpage\n",
    "html_page = requests.get(url).text\n",
    "data = BeautifulSoup(html_page, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "944d5df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrap the target table data\n",
    "tables = data.find_all('tbody')\n",
    "rows = tables[0].find_all('tr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e9868e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iterate over the content of the table rows and append each row to the dataframe\n",
    "\n",
    "for row in rows:\n",
    "    col = row.find_all('td')\n",
    "     \n",
    "    if len(col)!=0:\n",
    "        data_dict = {\"Job_Title\": col[0].contents[1],\n",
    "                     \"Job_Grade\": col[0].contents[2],\n",
    "                     \"No_Of_Positions\": col[0].contents[2],\n",
    "                    \"Reference_No\": col[0].contents[2]}\n",
    "        df1 = pd.DataFrame(data_dict, index=[0])\n",
    "        df = pd.concat([df,df1], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6396d7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Job_Title Job_Grade No_Of_Positions Reference_No\n",
      "0                                     \\n              \\n           \\n\n",
      "1                     1.              \\n              \\n           \\n\n",
      "2                     2.              \\n              \\n           \\n\n",
      "3                     3.              \\n              \\n           \\n\n",
      "4                     4.              \\n              \\n           \\n\n",
      "5   [NON – ACADEMIC POSITIONS]        \\n              \\n           \\n\n",
      "6                     5.              \\n              \\n           \\n\n",
      "7                     6.              \\n              \\n           \\n\n",
      "8                     7.              \\n              \\n           \\n\n",
      "9                     8.              \\n              \\n           \\n\n",
      "10                    9.              \\n              \\n           \\n\n",
      "11                      10.           \\n              \\n           \\n\n",
      "12                      11.           \\n              \\n           \\n\n",
      "13                      12.           \\n              \\n           \\n\n",
      "14                      13.           \\n              \\n           \\n\n",
      "15                      14.           \\n              \\n           \\n\n",
      "16                      15.           \\n              \\n           \\n\n",
      "17                      16.           \\n              \\n           \\n\n",
      "18                      17.           \\n              \\n           \\n\n",
      "19                      18.           \\n              \\n           \\n\n",
      "20                      19.           \\n              \\n           \\n\n",
      "21                      20.           \\n              \\n           \\n\n",
      "22                      21.           \\n              \\n           \\n\n",
      "23                      22.           \\n              \\n           \\n\n",
      "24                      23.           \\n              \\n           \\n\n"
     ]
    }
   ],
   "source": [
    "#View the scrapped data\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1680c192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['S/No', 'Position', 'Grade', 'Positions', 'Reference Number']\n",
      "['', 'ACADEMIC POSITIONS']\n",
      "['1.', 'Associate Professor', '14', '1', 'KarU/HR/AP/2024']\n",
      "['2.', 'Senior Lecturer', '13', '5', 'KarU/HR/SL/2024']\n",
      "['3.', 'Lecturer', '12', '20', 'KarU/HR/LEC/2024']\n",
      "['4.', 'Tutorial Fellow', '11', '3', 'KarU/HR/TF/2024']\n",
      "['NON – ACADEMIC POSITIONS']\n",
      "['5.', 'Senior Assistant Registrar (Human Resource)', '13', '1', 'KarU/HR/SAR/HR/2024']\n",
      "['6.', 'Corporate Communications Officer', '12', '1', 'KarU/HR/CCO/2024']\n",
      "['7.', 'Medical Officer', '12', '1', 'KarU/HR/MO/2024']\n",
      "['8.', 'Moodle Instructional Designer', '12', '1', 'KarU/HR/MID/2024']\n",
      "['9.', 'Internal Auditor I', '12', '1', 'KarU/HR/IA/2024']\n",
      "['10.', 'Senior Assistant Systems Librarian', '11', '1', 'KarU/HR/SASL/2024']\n",
      "['11.', 'Assistant Games Tutor I', '11', '1', 'KarU/HR/ AGT.I/2024']\n",
      "['12.', 'Dynamics 365 BC Backend Developer', '10', '1', 'KarU/HR/D365BC/2024']\n",
      "['13.', 'Medical Physiology Laboratory Technologist', '10', '1', 'KarU/HR/MPLT/2024']\n",
      "['14.', 'Human Anatomy Laboratory Technologist', '10', '1', 'KarU/HR/HALT/2024']\n",
      "['15.', 'Nursing Skills Laboratory Technician', '10', '1', 'KarU/HR/NSL/2024']\n",
      "['16.', 'Medical Laboratory Technologist', '10', '1', 'KarU/HR/MLT/2024']\n",
      "['17.', 'Clinical Medicine Instructor', '10', '1', 'KarU/HR/CMI/2024']\n",
      "['18.', 'Assistant Games Tutor II', '10', '1', 'KarU/HR/AGT.II/2024']\n",
      "['19.', 'Webmaster', '9', '1', 'KarU/HR/WM/2024']\n",
      "['20.', 'Computer Technician I', '8', '2', 'KarU/HR/CT/2024']\n",
      "['21.', 'Assistant Computer Technician II', '7', '2', 'KarU/HR/ ACT.II/2024']\n",
      "['22.', 'Assistant Computer Technician III', '6', '2', 'KarU/HR/ ACT.III/2024']\n",
      "['23.', 'Animal Health Technician', '6', '1', 'KarU/ HR/AHT/ 2024']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL of the webpage containing the table\n",
    "url = 'https://karu.ac.ke/past-jobs-karatina-university-page/'\n",
    "\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the webpage content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find the table in the webpage (assuming there is only one table)\n",
    "table = soup.find('table')\n",
    "\n",
    "# Extract the table headers\n",
    "headers = []\n",
    "for header in table.find_all('th'):\n",
    "    headers.append(header.text.strip())\n",
    "\n",
    "# Extract the table rows\n",
    "rows = []\n",
    "for row in table.find_all('tr'):\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 0:\n",
    "        row_data = [cell.text.strip() for cell in cells]\n",
    "        rows.append(row_data)\n",
    "\n",
    "# Print the headers and rows\n",
    "print(headers)\n",
    "for row in rows:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73e40f24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Write the headers and rows to a CSV file\n",
    "with open('Karu _jOBS_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(rows)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac4740",
   "metadata": {},
   "source": [
    "Explanation of the above code\n",
    "web scraping the table data from a webpage using `BeautifulSoup` and `requests` in Python.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "```\n",
    "- This imports the `requests` library, which allows us to send HTTP requests, and the `BeautifulSoup` class from the `bs4` module, which helps in parsing HTML and XML documents.\n",
    "\n",
    "```python\n",
    "# URL of the webpage containing the table\n",
    "url = 'URL_OF_THE_WEBPAGE'\n",
    "```\n",
    "- This defines the URL of the webpage that contains the table you want to scrape. You'll need to replace `'URL_OF_THE_WEBPAGE'` with the actual URL of the webpage.\n",
    "\n",
    "```python\n",
    "# Send a GET request to the webpage\n",
    "response = requests.get(url)\n",
    "```\n",
    "- This sends a GET request to the specified URL and stores the server's response in the `response` object.\n",
    "\n",
    "```python\n",
    "# Parse the webpage content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "```\n",
    "- This parses the content of the response (the HTML of the webpage) using `BeautifulSoup`. The `html.parser` is used to parse the HTML content.\n",
    "\n",
    "```python\n",
    "# Find the table in the webpage (assuming there is only one table)\n",
    "table = soup.find('table')\n",
    "```\n",
    "- This finds the first `<table>` element in the parsed HTML. It assumes there is only one table on the webpage. If there are multiple tables, you'll need to modify this to select the correct one.\n",
    "\n",
    "```python\n",
    "# Extract the table headers\n",
    "headers = []\n",
    "for header in table.find_all('th'):\n",
    "    headers.append(header.text.strip())\n",
    "```\n",
    "- This initializes an empty list `headers`. It then iterates over all `<th>` (table header) elements found within the table and appends their text content (with leading and trailing whitespace removed) to the `headers` list.\n",
    "\n",
    "```python\n",
    "# Extract the table rows\n",
    "rows = []\n",
    "for row in table.find_all('tr'):\n",
    "    cells = row.find_all('td')\n",
    "    if len(cells) > 0:\n",
    "        row_data = [cell.text.strip() for cell in cells]\n",
    "        rows.append(row_data)\n",
    "```\n",
    "- This initializes an empty list `rows`. It iterates over all `<tr>` (table row) elements found within the table. For each row, it finds all `<td>` (table data) elements. If the row contains any `<td>` elements (i.e., it is not a header row), it creates a list `row_data` containing the text content of each cell (with leading and trailing whitespace removed) and appends this list to `rows`.\n",
    "\n",
    "```python\n",
    "# Print the headers and rows\n",
    "print(headers)\n",
    "for row in rows:\n",
    "    print(row)\n",
    "```\n",
    "- This prints the list of headers and each row of data to the console.\n",
    "\n",
    "```python\n",
    "import csv\n",
    "\n",
    "# Write the headers and rows to a CSV file\n",
    "with open('table_data.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(headers)\n",
    "    writer.writerows(rows)\n",
    "```\n",
    "- This imports the `csv` module, which provides functionality to work with CSV files. It then opens a new CSV file named `table_data.csv` in write mode. A `csv.writer` object is created, and the `writer.writerow(headers)` writes the headers to the first row of the CSV file. The `writer.writerows(rows)` writes all the data rows to the CSV file. The `with open(...)` context manager ensures that the file is properly closed after writing.\n",
    "\n",
    "In summary, this script fetches the HTML content of a webpage, parses it to find the table, extracts the headers and rows of the table, prints them to the console, and writes them to a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21c7030",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1533cab2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
